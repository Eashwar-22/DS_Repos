### Implementing Gradient Descent on a given Cost function and a starting point.

- The Objective is to find the local minima of a cost function based on various hyper parameters that are passed as the arguments.
- The function can get to the minimum cost function, whether we have multiple local minima in the curve or not. This is dependent on the starting point of iteration - the point being located at the positive/negative gradient(slope).
- The notebook initially describes the implementation of the descent for a simple cost function and then a user defined function is created in order to accomodate any linear/non-linear expression; also plotting the curves.

>>> ![GD_curve](GD_curve.jpeg)
